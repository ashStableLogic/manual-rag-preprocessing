{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/rag/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings as Embedder\n",
    "\n",
    "from langchain_text_splitters import MarkdownTextSplitter as TextSplitter\n",
    "\n",
    "from pgvector.psycopg2 import register_vector\n",
    "import psycopg2\n",
    "\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "import base64\n",
    "\n",
    "import torch\n",
    "\n",
    "import os\n",
    "\n",
    "import requests\n",
    "\n",
    "VISION_MODEL_NAME = \"llava-hf/llava-1.5-7b-hf\"\n",
    "\n",
    "###EXTRACTION MINIMUM PX SIZES\n",
    "MIN_IMG_PX_HEIGHT = 100\n",
    "MIN_IMG_PX_WIDTH = 100\n",
    "\n",
    "\n",
    "class Database(object):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "\n",
    "        self.embedder = Embedder()\n",
    "\n",
    "        self.document_insert_string = \"INSERT INTO documents (document_name,document_type) VALUES (%s,%s) RETURNING document_id;\"\n",
    "        self.chunk_insert_string = \"INSERT INTO chunks (document_id, chunk, embedding) VALUES (%s, %s, %s) RETURNING chunk_id;\"\n",
    "        self.image_insert_string = \"INSERT INTO images (chunk_id,image_name,image_filepath,image_summary,embedding) VALUES (%s,%s,%s,%s,%s) RETURNING image_id\"\n",
    "\n",
    "        self.conn = psycopg2.connect(\n",
    "            host=\"localhost\", database=\"at_docs\", user=\"postgres\", password=\"password\"\n",
    "        )\n",
    "\n",
    "        self.cursor = self.conn.cursor()\n",
    "\n",
    "        self.cursor.execute(\"CREATE EXTENSION IF NOT EXISTS vector\")\n",
    "\n",
    "        # self.cursor.execute(\"CREATE INDEX IF NOT EXISTS ON chunks USING hnsw (embedding vector_l2_ops)\")\n",
    "        # Make an index AFTER loading in tons of data.\n",
    "\n",
    "        ###index will use L2 distance (RMS), so use <-> operator ONLY in select statements (at least for chunks ^)\n",
    "\n",
    "        self.conn.commit()\n",
    "\n",
    "        register_vector(conn_or_curs=self.cursor)\n",
    "\n",
    "        self.text_splitter = TextSplitter(\n",
    "            chunk_size=1000, chunk_overlap=200, length_function=len\n",
    "        )\n",
    "\n",
    "        self.embedder = Embedder()\n",
    "\n",
    "        return\n",
    "    \n",
    "    def init_image_summary_model(self):\n",
    "    \n",
    "        self.llava_image_summary_prompt = \"USER: <image>\\nSummarize this image using the following context. {context}\\nASSISTANT: \"\n",
    "\n",
    "        self.vision_model = LlavaForConditionalGeneration.from_pretrained(\n",
    "            VISION_MODEL_NAME,\n",
    "            low_cpu_mem_usage=True,\n",
    "            torch_dtype=torch.float16,\n",
    "            load_in_4bit=True\n",
    "        )\n",
    "        \n",
    "        self.vision_processor=AutoProcessor.from_pretrained(VISION_MODEL_NAME)\n",
    "        \n",
    "        return\n",
    "\n",
    "    def get_b64_image(self, image_path: str) -> str:\n",
    "\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "    def summarise_image(self, image_path: str, context: str) -> dict:\n",
    "\n",
    "        question = self.llava_image_summary_prompt.format(context=context)\n",
    "\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        inputs=self.vision_processor(question,image,return_tensors=\"pt\").to(0,torch.float16)\n",
    "\n",
    "        output = self.vision_model.generate(\n",
    "            **inputs,max_new_tokens=100,do_sample=False\n",
    "        )\n",
    "\n",
    "        response=self.vision_processor.decode(output[0][2:],skip_special_tokens=True)\n",
    "        \n",
    "        answer=response.partition(\"ASSISTANT:\")[-1].replace(\"\\n\",\"\")\n",
    "\n",
    "        return answer\n",
    "\n",
    "    def store_content(\n",
    "        self,\n",
    "        document_text: str,\n",
    "        absolute_document_path: str,\n",
    "        document_name: str,\n",
    "        document_type: str,\n",
    "    ) -> None:\n",
    "\n",
    "        folder_path = os.path.dirname(absolute_document_path)\n",
    "\n",
    "        document_data_to_insert = (\n",
    "            document_name,\n",
    "            document_type,\n",
    "        )\n",
    "\n",
    "        self.cursor.execute(\n",
    "            self.document_insert_string,\n",
    "            document_data_to_insert,\n",
    "        )\n",
    "\n",
    "        document_id = self.cursor.fetchone()[0]\n",
    "\n",
    "        self.conn.commit()\n",
    "\n",
    "        chunks = self.text_splitter.split_text(document_text)\n",
    "\n",
    "        embeddings = self.embedder.embed_documents(chunks)\n",
    "\n",
    "        for embedding, chunk in zip(embeddings, chunks):\n",
    "\n",
    "            embedding = np.array(embedding)\n",
    "\n",
    "            chunk_data_to_insert = (\n",
    "                document_id,\n",
    "                chunk,\n",
    "                embedding,\n",
    "            )\n",
    "\n",
    "            self.cursor.execute(self.chunk_insert_string, chunk_data_to_insert)\n",
    "\n",
    "            chunk_id = self.cursor.fetchone()[0]\n",
    "\n",
    "            image_paths_in_chunk = [\n",
    "                os.path.join(folder_path, image_name)\n",
    "                for image_name in re.findall(r\"\\[(.*\\.png)\\]\\(\\1\\)\", chunk)\n",
    "            ]\n",
    "\n",
    "            self.conn.commit()\n",
    "\n",
    "            for image_path in image_paths_in_chunk:\n",
    "\n",
    "                height, width = cv2.imread(image_path, 0).shape[:2]\n",
    "\n",
    "                if height >= MIN_IMG_PX_HEIGHT and width > MIN_IMG_PX_WIDTH:\n",
    "\n",
    "                    image_summary = self.summarise_image(image_path, chunk)\n",
    "\n",
    "                    image_summary_embedding = self.embedder.embed_query(image_summary)\n",
    "\n",
    "                    image_data_to_insert = (\n",
    "                        chunk_id,\n",
    "                        os.path.basename(image_path)[:-4],\n",
    "                        image_path,\n",
    "                        image_summary,\n",
    "                        image_summary_embedding,\n",
    "                    )\n",
    "\n",
    "                    self.cursor.execute(self.image_insert_string, image_data_to_insert)\n",
    "\n",
    "                    self.conn.commit()\n",
    "\n",
    "        return\n",
    "\n",
    "    def get_k_relavent_chunks(self, question: str, k_num: int = 5) -> list[tuple[str]]:\n",
    "\n",
    "        question_select_string = f\"SELECT chunk_id,chunk FROM chunks ORDER BY embedding <-> (%s) LIMIT {k_num}\"\n",
    "\n",
    "        embedded_question = np.array(self.embedder.embed_query(question))\n",
    "\n",
    "        data_to_insert = (embedded_question,)\n",
    "\n",
    "        self.cursor.execute(question_select_string, data_to_insert)\n",
    "\n",
    "        top_k_chunks = self.cursor.fetchall()\n",
    "\n",
    "        return top_k_chunks\n",
    "\n",
    "    def get_most_relavent_chunk(self, question) -> tuple[str]:\n",
    "        return self.get_k_relavent_chunks(question, k_num=1)[0]\n",
    "\n",
    "    def get_most_relevant_image_paths_and_summaries(\n",
    "        self, question: str, chunk_ids: list[str], k_num: int = 5\n",
    "    ) -> list[tuple[str]]:\n",
    "\n",
    "        question_select_string = f\"SELECT image_id,image_filepath,image_summary FROM images WHERE chunk_id IN (%s) ORDER BY embedding <-> (%s) LIMIT {k_num}\"\n",
    "\n",
    "        embedded_question = np.array(self.embedder.embed_query(question))\n",
    "\n",
    "        data_to_insert = (\n",
    "            chunk_ids,\n",
    "            embedded_question,\n",
    "        )\n",
    "\n",
    "        self.cursor.execute(question_select_string, data_to_insert)\n",
    "\n",
    "        top_k_images_and_summaries = self.cursor.fetchall()\n",
    "\n",
    "        return top_k_images_and_summaries\n",
    "\n",
    "    def get_most_relevant_image_paths_and_summary(\n",
    "        self, question: str, chunk_id: str\n",
    "    ) -> tuple[str]:\n",
    "\n",
    "        return self.get_most_relevant_image_paths_and_summaries(\n",
    "            question, tuple([chunk_id]), k_num=1\n",
    "        )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "/home/ubuntu/miniconda3/envs/rag/lib/python3.10/site-packages/transformers/models/llava/configuration_llava.py:100: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:34<00:00, 11.41s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "db = Database()\n",
    "\n",
    "db.init_image_summary_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = (\n",
    "    \"/home/ubuntu/manual-rag-preprocessing/test_documents/CDJ-3000_manual_EN_ONE_PAGE.pdf-0-0.png\"\n",
    ")\n",
    "\n",
    "context = \"\"\"## Instruction Manual\n",
    "\n",
    "![CDJ-3000_manual_EN_ONE_PAGE.pdf-0-0.png](CDJ-3000_manual_EN_ONE_PAGE.pdf-0-0.png)\n",
    "\n",
    "### Multi player\n",
    "# CDJ-3000\n",
    "\n",
    "**[pioneerdj.com/support/](https://www.pioneerdj.com/support/)**\n",
    "\n",
    "**[rekordbox.com](https://rekordbox.com/)**\n",
    "\n",
    "[For FAQs and other support information for this product, visit the websites above.](https://kuvo.com/)\n",
    "\n",
    "\n",
    "-----\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/rag/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:426: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n",
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    }
   ],
   "source": [
    "answer=db.summarise_image(image_path,context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The image is a detailed diagram of a CDJ-3000, a multi-player device. The diagram is a combination of a flowchart and a wiring diagram, providing a clear visual representation of the device's components and connections. The diagram is labeled with various terms and phrases, such as \"loop,\" \"loop out,\" \"loop in,\" \"loop back,\" \"loop bypass,\" and \"loop bypass out.\" The diagram also includes a clock\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
